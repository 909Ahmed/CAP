{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from data2 import DaVaDataset\n",
    "from PIL import Image\n",
    "import pickle\n",
    "from io import BytesIO\n",
    "import clip\n",
    "\n",
    "clipped, clippreprocess = clip.load(\"ViT-B/32\", device='cuda')\n",
    "data = pickle.load(open('./data/images.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from utils import get_phi_model\n",
    "from model import DaVa\n",
    "import torch\n",
    "\n",
    "phi_model = get_phi_model()\n",
    "model = DaVa(phi_model=phi_model)\n",
    "# model.apply_lora_to_proj(64, 128)\n",
    "\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "state_dict1 = torch.load('./weights/final_states26000.pkl')\n",
    "state_dict2 = torch.load('./weights/final_statues26000.pkl')\n",
    "\n",
    "model.proj.load_state_dict(state_dict1)\n",
    "model.phi.load_state_dict(state_dict2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from torch.cuda.amp import autocast\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "promptpre = \"<|system|>\\nYou are a helpful assistant.<|end|>\\n<|user|>\\n<Image>\"\n",
    "promptpre = model.tokenizer(promptpre, return_tensors=\"pt\", add_special_tokens=False).to('cuda')\n",
    "with autocast():\n",
    "    image_embeds = model.proj(image)\n",
    "promptpost = \"What can you infer about the environment or setting of the image? <|end|>\\n<|assistant|>\\n\"\n",
    "promptpost = model.tokenizer(promptpost, return_tensors=\"pt\", add_special_tokens=False).to('cuda')\n",
    "\n",
    "bos_embeds = model.phi.model.embed_tokens(torch.tensor([model.tokenizer.bos_token_id])).expand(1, -1, -1)  # bsz x 1 x embed_dim\n",
    "p_before_embeds = model.phi.model.embed_tokens(promptpre.input_ids).expand(1, -1, -1) # 1 x s1 x embed_dim\n",
    "p_after_embeds = model.phi.model.embed_tokens(promptpost.input_ids).expand(1, -1, -1)  # 1 x s2 x embed_dim\n",
    "\n",
    "combined_embeds = torch.cat([bos_embeds, p_before_embeds, image_embeds, p_after_embeds], dim=1).to('cuda')\n",
    "\n",
    "max_length = 30\n",
    "\n",
    "predicted_caption = torch.full((1, max_length), 32007)\n",
    "\n",
    "for pos in tqdm(range(max_length - 1)):\n",
    "\n",
    "    with autocast():\n",
    "        model_output_logits = model.phi.forward(inputs_embeds = combined_embeds)['logits'] # 4,49,51200\n",
    "    predicted_word_token_logits = model_output_logits[:, -1, :].unsqueeze(1) # 4,1,51200\n",
    "    predicted_word_token = torch.argmax(predicted_word_token_logits, dim = -1) # 4,1\n",
    "    predicted_caption[:,pos] = predicted_word_token.view(1,-1).to('cpu')\n",
    "    next_token_embeds = model.phi.model.embed_tokens(predicted_word_token) # 4,1,2560\n",
    "    combined_embeds   = torch.cat([combined_embeds, next_token_embeds], dim=1)\n",
    "\n",
    "    if predicted_word_token == 32007:\n",
    "        break\n",
    "\n",
    "    del model_output_logits\n",
    "    del predicted_word_token_logits\n",
    "    del predicted_word_token\n",
    "    del next_token_embeds\n",
    "\n",
    "print(model.tokenizer.decode(predicted_caption[0]))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
